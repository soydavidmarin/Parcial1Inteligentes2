# -*- coding: utf-8 -*-
"""Parcial1Inteligentes2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AooS6FdMjMDPUfEUgzF4kKDil06EfdNn
"""

import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import colors
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import seaborn as sb
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn import metrics


from scipy.stats import shapiro
from scipy.stats import normaltest
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import GaussianNB

from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import classification_report
from sklearn.neighbors import KNeighborsClassifier

from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
import graphviz

from sklearn.svm import SVC

from google.colab import files
import io


#Subida de datasets
archivo=files.upload()
archivo2=files.upload()
dataframe=pd.read_csv(io.BytesIO(archivo['bank-full.csv']),delimiter=";")
dataframeLite=pd.read_csv(io.BytesIO(archivo2['bank.csv']),delimiter=";")
dataframe.head(7)

#Estadisticas 
totalData = len(dataframe.index)
print(dataframe.groupby("y").size())
print("%No: ",(dataframe.groupby("y").size()["no"]/totalData)*100,"%")
print("%Si: ",(dataframe.groupby("y").size()["yes"]/totalData)*100,"%")
print("Total: ", totalData)

#Eliminación de columnas posiblemente inutiles (Elegidas por los autores)
dataframeLite=dataframeLite.drop(["contact", "pdays"],axis=1)
dataframe=dataframe.drop(["contact", "pdays"],axis=1)
dataframe.head(7)

#Columnas con distribución normal
columnas=dataframe.columns.values
normal=[]
noNormal=[]
for columnaActual in columnas:
  if dataframe[columnaActual].dtypes == object or dataframe[columnaActual].dtypes == bool:    
    dataframe[columnaActual]=LabelEncoder().fit_transform(dataframe[columnaActual])    
    dataframeLite[columnaActual]=LabelEncoder().fit_transform(dataframeLite[columnaActual])
 
  datosColumna=dataframeLite[columnaActual]
  #print(datosColumna.shape)
  stat,p=shapiro(datosColumna)
  #print("stat=",stat," p=",p)
  if p>0.05:
    normal.append(columnaActual)
  else:
    noNormal.append(columnaActual)
  #print("--------------")
print("Con distribucion normal: ",normal)
print("Sin distribucion normal: ",noNormal)
dataframe.head(7)

#Matriz de correlación
colormap = plt.cm.coolwarm
plt.figure(figsize=(12,12))
plt.title('Matriz de correlación', y=1.05, size=15)
sb.heatmap(dataframe.drop(['y'], axis=1).astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)

#Graficas de las distribuciones
plt.rcParams['figure.figsize'] = (16, 9)
plt.style.use('ggplot')

dataframe.drop(["y"],axis=1).hist()
plt.show()


#Histogramas
columnas=list(dataframe.drop(["y"],axis=1).columns)
for columnaActual in columnas:
  no = dataframe[dataframe["y"] == 0]
  si = dataframe[dataframe["y"] == 1]

  columnaAnalizadaVender=no[columnaActual].to_numpy()
  columnaAnalizadaComprar=si[columnaActual].to_numpy()
  
  clases = ['no', 'si']

  sb.distplot(columnaAnalizadaVender, hist = True, kde = True, kde_kws = {'linewidth': 3}, label = clases[0])
  sb.distplot(columnaAnalizadaComprar, hist = True, kde = True, kde_kws = {'linewidth': 3}, label = clases[1])
      
  # Plot formatting
  plt.legend(prop={'size': 16}, title = 'Categorias')
  plt.title('Distribution Plot '+columnaActual)
  plt.xlabel('Data')
  plt.ylabel('Distribution')
  plt.show()


#Particion del dataset
X=dataframe.drop(["y"],axis=1)
y=dataframe["y"]
XTrain,XTest,yTrain,yTest = None,None,None,None

def partition(test_size):
  XTrain,XTest,yTrain,yTest=train_test_split(X,y,test_size=test_size)


"""Entrenamiento de modelos

Gaussian Naive Bayes
"""

modeloGNB=GaussianNB()
modeloGNB.fit(XTrain,yTrain)
yPredictGNB=modeloGNB.predict(XTest)
print(yPredictGNB)


matrizGNB=confusion_matrix(yTest,yPredictGNB)
print(matrizGNB)
sb.heatmap(matrizGNB,annot=True,cmap="Blues")
precision_recall_fscore_support(yTest,yPredictGNB,average=None)
etiquetas=["no","si"]
print(classification_report(yTest,yPredictGNB,target_names=etiquetas))
print("Accuracy=",modeloGNB.score(XTest,yTest))



"""KNN"""

distancias=["","manhattan","euclidean","minkowski"]
for k in [1,5,7,33,55,77,128]:
  for a in range(1,len(distancias)):
    modeloKNN=KNeighborsClassifier(n_neighbors=k,p=a)    
    modeloKNN.fit(XTrain,yTrain)
    yPredictKNN=modeloKNN.predict(XTest)
    print("Experimento k=",k," distancia =",distancias[a]," Accuracy=",metrics.accuracy_score(yTest,yPredictKNN))
  print("---------------------------------------")

modeloKNN=KNeighborsClassifier(n_neighbors=77,p=3)
modeloKNN.fit(XTrain,yTrain)
yPredictKNN=modeloKNN.predict(XTest)
print("k = 77"," distancia = minkowski"," Accuracy = ",metrics.accuracy_score(yTest,yPredictKNN))

matrizKNN=confusion_matrix(yTest,yPredictKNN)
print(matrizKNN)
sb.heatmap(matrizKNN,annot=True,cmap="Blues")

precision_recall_fscore_support(yTest,yPredictKNN,average=None)
etiquetas=["no","si"]
print(classification_report(yTest,yPredictKNN,target_names=etiquetas))
print("Accuracy=",modeloKNN.score(XTest,yTest))

"""Arboles de decisión """

modeloDTC=DecisionTreeClassifier()
modeloDTC.fit(XTrain,yTrain)
yPredictDTC=modeloDTC.predict(XTrain)
print("Depth = Default,","Accuracy =",modeloDTC.score(XTest,yTest))
max = 0
imax = 0
for i in range(1,100):
  modeloDTC=DecisionTreeClassifier(max_depth=i)
  modeloDTC.fit(XTrain,yTrain)
  yPredictDTC=modeloDTC.predict(XTrain)
  accuracy = modeloDTC.score(XTest,yTest)  
  if accuracy > max:
    max = accuracy
    imax = i
print("iMax:",imax," Max:",max)
modeloDTC=DecisionTreeClassifier(max_depth=imax)
modeloDTC.fit(XTrain,yTrain)

yPredictDTC=modeloDTC.predict(XTest)
print("Accuracy ",metrics.accuracy_score(yTest,yPredictDTC))

matrizDTC=confusion_matrix(yTest,yPredictDTC)
print(matrizDTC)
sb.heatmap(matrizDTC,annot=True,cmap="Blues")

precision_recall_fscore_support(yTest,yPredictDTC,average=None)
etiquetas=["no","si"]
print(classification_report(yTest,yPredictDTC,target_names=etiquetas))
print("Accuracy=",modeloDTC.score(XTest,yTest))


caracteristicas = dataframe.drop(['y'], axis=1)
caracteristicas = list(caracteristicas.columns)
etiquetas=["no","si"]
export_graphviz(modeloDTC,out_file="arbol.dot",class_names=etiquetas,feature_names=caracteristicas,impurity=False,filled=True)
with open("arbol.dot") as f:
  dot_graph=f.read()
graphviz.Source(dot_graph)

plt.barh(range(len(caracteristicas)),modeloDTC.feature_importances_)
plt.yticks(np.arange(len(caracteristicas)),caracteristicas)
plt.xlabel("Importancia de las características")
plt.ylabel("Características")
plt.show()

"""Maquinas de soporte vectorial"""

modeloSVC=SVC()
modeloSVC.fit(XTrain,yTrain)
yPredictSVC=modeloSVC.predict(XTest)
print("Accruracy =",metrics.accuracy_score(yTest,yPredictSVC))

matrizSVC=confusion_matrix(yTest,yPredictSVC)
print(matrizSVC)
sb.heatmap(matrizSVC,annot=True,cmap="Blues")

precision_recall_fscore_support(yTest,yPredictSVC,average=None)
etiquetas=["no","si"]
print(classification_report(yTest,yPredictSVC,target_names=etiquetas))
print("Accuracy=",modeloSVC.score(XTest,yTest))